[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lego II",
    "section": "",
    "text": "Este curso tem por objetivo fornecer um panorama de diversos Modelos de Regressão e suas aplicações nas ciências sociais. Na Parte 1, abordaremos o Modelo de Regressão Linear Múltipla. Revisaremos suas características básicas nas primeiras aulas, aprofundando a compreensão de seus pressupostos e propriedades matemáticas. Na Parte 2, trataremos do problema da incerteza e da inferência estatística em modelos de regressão. Na Parte 3, veremos extensões dos modelos de regressão, para modelar relações não lineares, fazer predições e extrapolações. Na Parte 4, abordaremos modelos para dados categóricos ou censurados. Na Parte 5, abordaremos modelos para dados em painel/longitudinais (i.e. nos quais observamos unidades analíticas ao longo do tempo) [ementa]\n\nProfessor: Rogério Jerônimo Barbosa (IESP-UERJ) [homepage]\nSemestre: 2025.2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Apresentação do curso</span>"
    ]
  },
  {
    "objectID": "aulas/nivelamento.html",
    "href": "aulas/nivelamento.html",
    "title": "2  Nivelamento",
    "section": "",
    "text": "A disciplina de Lego II inicia com aulas intensivas de nivelamento em matemática ao longo de uma semana. As aulas trataram dos fundamentos de álgebra linear (Strang 2016) e cálculo.\n\nAula 1: vetores e a ideia da combinação linear [pdf]\nAula 2: dependência linear e resolvendo \\(A x = b\\) por Gauss-Jordan [pdf]\nAula 3: produto escalar, norma de vetores, ortogonalidade e multiplicação de matrizes [pdf]\nAula 4: matrizes especiais – transposta, identidade, simétrica e inversa [pdf]\nAula 5: funções, limite e um pouco de derivada [pdf]\n\nRetomamos e aprofundamos alguns temas importantes de cálculo na aula 2 do curso. [pdf]\nEnunciado e solução da lista de nivelamento em matemática.\n\n\n\n\nStrang, Gilbert. 2016. Introduction to Linear Algebra. 5th ed. Wellesley, MA: Wellesley-Cambridge Press.",
    "crumbs": [
      "Nivelamento",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Nivelamento</span>"
    ]
  },
  {
    "objectID": "aulas/aula-1.html",
    "href": "aulas/aula-1.html",
    "title": "3  Derivando os estimadores da regressão",
    "section": "",
    "text": "Wooldridge (2020), chapter 2\n\\[\ny = \\beta_0 + \\beta_1 x + u\n\\]\nThis is a statement about the distribution of the unobserved factors in the population.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Derivando os estimadores da regressão</span>"
    ]
  },
  {
    "objectID": "aulas/aula-1.html#deriving-the-ols-estimates-using-the-method-of-moments",
    "href": "aulas/aula-1.html#deriving-the-ols-estimates-using-the-method-of-moments",
    "title": "3  Derivando os estimadores da regressão",
    "section": "3.1 Deriving the OLS estimates using the method of moments",
    "text": "3.1 Deriving the OLS estimates using the method of moments\nNow, let’s proceed to derive the ordinary least squares estimates. Of course, I’m just going to describe the step-by-step specified in the section 2.2 of Wooldridge (2020). Assume we have a random sample \\(\\{ (x_i, y_i): i = 1, ..., n \\}\\). We can write a simple regression model as follows:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + u_1\n\\]\nfor each \\(i\\).\nThere are many ways to motivate this procedure; let’s use the following approach. It is an assumption of the linear model that \\(\\mathbb{E}[u] = 0\\), what also means that the covariance between \\(x\\) and \\(u\\) is zero – \\(\\text{Cov}(x, u) = \\mathbb{E}[xu] = 0\\). That being said, we can write these two equations below:\n\\[\n\\begin{align*}\n\\mathbb{E}[u_i] = 0 &\\Longrightarrow \\mathbb{E}[y - \\beta_0 - \\beta_1 x] = 0 \\\\\n\\mathbb{E}[xu] = 0 &\\Longrightarrow \\mathbb{E}[x(y - \\beta_0 - \\beta_1 x)] = 0\n\\end{align*}\n\\]\nThese two equations imply two restrictins on the joint probability distribution of \\(\\{x, y\\}\\) in the population. There are two unknown parameters to estimate, so we might hope that they can be used to obtain good estimators of \\(\\beta_0\\) and \\(\\beta_1\\). In fact, given a sample of data, we can choose estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to solve the sample counterparts of these two equations:\n\\[\n\\begin{align*}\n\\mathbb{E}[u_i] = 0 &\\Longrightarrow \\frac{1}{n} \\sum^n_{i = 1} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n\\mathbb{E}[xu] = 0 &\\Longrightarrow \\frac{1}{n} \\sum^n_{i = 1} x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0\n\\end{align*}\n\\]\nThis is an example of the method of moments approach (Wooldridge 2020, 25). In fact, the idea of the method of moments is that the population distribution has some theoretical moments (the mean or the variance, for example), and we estimate parameters by equating these theoretical moments with the corresponding sample moments.\nWe can rewrite the first equation as\n\\[\n\\begin{align*}\n& \\bar{y} - \\hat{\\beta}_0 - \\hat{\\beta}_1 \\bar{x} = 0 \\Rightarrow  \\\\\n&\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{align*}\n\\]\nWe can then plug this result into the second equations (ignoring \\(n^{-1}\\), which makes no difference in the equality), what gives us\n\\[\n\\begin{align*}\n&\\sum^n_{i = 1} x_i [y_i - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) - \\hat{\\beta}_1 x_i] = 0 \\Rightarrow \\\\\n&\\sum^n_{i = 1} x_i (y_i - \\bar{y}) = \\hat{\\beta}_1 \\sum^n_{i = 1} x_i (x_i - \\bar{x})\n\\end{align*}\n\\]\nFrom the properties of summation, it follows that\n\\[\n\\begin{align*}\n&\\sum^n_{i = 1} x_i (y_i - \\bar{y}) = \\sum^n_{i = 1} (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\n&\\sum^n_{i = 1} x_i (x_i - \\bar{x}) = \\sum^n_{i = 1} (x_i - \\bar{x})^2\n\\end{align*}\n\\]\nSo, provided that \\(\\sum^n_{i = 1} (x_i - \\bar{x})^2 &gt; 0\\),\n\\[\n\\hat{\\beta}_1 = \\dfrac{\\sum^n_{i = 1} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum^n_{i = 1} (x_i - \\bar{x})^2}\n\\]\n\nEquation (2.19) [this last one] is simply the sample covariance between \\(x_i\\) and \\(y_i\\) divided by the sample variance of \\(x_i\\). Using simple algebra we can also write \\(\\hat{\\beta}_1\\) as\n\\[\n\\hat{\\beta}_1 = \\hat{\\rho}_{xy} \\cdot \\left( \\frac{ \\hat{\\sigma_y} }{ \\hat{\\sigma_x} } \\right),\n\\]\nwhere \\(\\hat{\\rho}_{xy}\\) is the sample correlation between \\(x_i\\) and \\(y_i\\) and \\(\\hat{\\sigma_x}, \\hat{\\sigma_y}\\) denote the sample standard deviations. […]. An immediate implication is that if \\(x_i\\) and \\(y_i\\) are positively correlated in the sample then \\(\\hat{\\beta}_1 &gt; 0\\); if \\(x_i\\) and \\(y_i\\) are negatively correlated then \\(\\hat{\\beta}_1 &lt; 0\\).\n[…]\n[…] Recognition that \\(\\beta_1\\) is just a scaled version of \\(\\rho_{xy}\\) highlights an important limitation of simple regression when we do not have experimental data: in effect, simple regression is an analysis of correlation between two variables, and so one must be careful in inferring causality. (Wooldridge 2020, 26)\n\nThe \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) estimates are the ordinary least squares (OLS) estimates of \\(\\beta_0\\) and \\(\\beta_1\\). These are the estimates that make the sum of the squared residuals as small as possible.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Derivando os estimadores da regressão</span>"
    ]
  },
  {
    "objectID": "aulas/aula-1.html#some-properties",
    "href": "aulas/aula-1.html#some-properties",
    "title": "3  Derivando os estimadores da regressão",
    "section": "3.2 Some properties",
    "text": "3.2 Some properties\n\n\\(\\sum_{i = 1}^n \\hat{u}_i = 0\\)\n\\(\\sum_{i = 1}^n x_i \\hat{u}_i = 0\\)\nThe point \\((\\bar{x}, \\bar{y})\\) is always on the OLS regression line.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Derivando os estimadores da regressão</span>"
    ]
  },
  {
    "objectID": "aulas/aula-1.html#errors",
    "href": "aulas/aula-1.html#errors",
    "title": "3  Derivando os estimadores da regressão",
    "section": "3.3 Errors",
    "text": "3.3 Errors\nDefine the total sume of squares (SST), the explained sum of squares (SSE), and the residual sum of squares (SSR):\n\\[\n\\begin{align*}\n&\\text{SST} \\equiv \\sum^n_{i = 1} (y_i - \\bar{y})^2 \\\\\n&\\text{SSE} \\equiv \\sum^n_{i = 1} (\\hat{y}_i - \\bar{y})^2 \\\\\n&\\text{SSR} \\equiv \\sum^n_{i = 1} \\hat{u}_i^2 = \\sum^n_{i = 1} (y_i - \\hat{y}_i)\n\\end{align*}\n\\]\nSST is a measure of the total sample variation in the \\(y_i\\), while SSE is a measure of the sample variation in the \\(\\hat{y}_i\\) and SSR measures the sample variation in the \\(\\hat{u}_i\\). The total variation can always be expressed as the sum of the explained variation and the unexplained variation SSR, that is, SST = SSE + SSR.\nThe coefficient of determination measures the percentage of the explained variance:\n\\[\nR^2 = \\frac{ \\text{SSE} }{ \\text{SST} } = 1 - \\frac{ \\text{SSR} }{ \\text{SST} }\n\\]",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Derivando os estimadores da regressão</span>"
    ]
  },
  {
    "objectID": "aulas/aula-1.html#pressupostos-do-modelo-linear-ou-os-supostos-de-gauss-markov",
    "href": "aulas/aula-1.html#pressupostos-do-modelo-linear-ou-os-supostos-de-gauss-markov",
    "title": "3  Derivando os estimadores da regressão",
    "section": "3.4 Pressupostos do modelo linear, ou os Supostos de Gauss-Markov",
    "text": "3.4 Pressupostos do modelo linear, ou os Supostos de Gauss-Markov\nML.1: Linearidade\n\\[\n\\mathbb{E}[Y | X] = f(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k\n\\]\nML.2: Os casos do seu banco de dados foram gerados pelo mesmo processo gerador e são independentes e identicamente distribuídos (iid).\nML.3: \\(X^TX\\) é invertível\nML.4: o vetor de erros é ortogonal (i.e., \\(\\text{cos}(\\theta) = 0\\)) aos vetores-coluna que compõem a matriz \\(X\\). Isto é: os betas captam o efeito de \\(X\\) assumindo tudo mais constante (ceteris paribus). Isso quase certamente é falso.\nSatisfeitos todos esses pressupostos, a regressão oferece o efeito causal; no entanto, como normalmente não satisfazemos todos eles, o resultado é outra coisa. O que isso significa exatamente é o tema do curso.\n\n\n\n\nWooldridge, Jeffrey M. 2020. Introductory Econometrics: A Modern Approach. 7th ed. Boston, MA: Cengage Learning.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Derivando os estimadores da regressão</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html",
    "href": "aulas/aula-2.html",
    "title": "4  Um pouco mais de cálculo",
    "section": "",
    "text": "Introduction to Calculus and the Derivative, Moore and Siegel (2013)\nSolução de alguns dos exercícios do capítulo.\n\nDifferential calculus thus deals with the study of infinitesimally small changes in a function. As we’ll see in Section 3 below, the derivative of the function \\(f(x) = 3x\\) is \\(3\\), which is the slope (rate of change) of the line \\(y = 3x\\). This example illustrates the way in which a derivative breaks down functions, removing information about their value at any point and providing just the value of the change at that point. (Moore and Siegel 2013, 104)\n\n\nWe put off until Chapter 11 of Part III of this book the most common use of integrals: in probability, specifically continuous probability distributions. There the integral will allow us to understand statistical inference with continuous variables and to compute expected values and expected utilities, which are vital when considering uncertainty of any sort in game theory. (Moore and Siegel 2013, 105)\n\n\nDiscrete change, then, is the first difference between two observations [the change from time \\(t - 1\\) to time \\(t\\); in percentagem terms, \\(\\frac{x_{t+1} - x_t}{x_t} \\times 100\\%\\)]. It is a measure of change in a variable across two discrete moments in time. It follows that the size of a first difference is going to vary across different temporal scales. (Moore and Siegel 2013, 105)\n\n\n[…] if we could evaluate the rate of change at a point on a function by taking the limit of the difference as the interval gets smaller and smaller, then that would tell us what the instantaneous rate of change was at the point (or moment in time). And that is precisely what the derivative does. (Moore and Siegel 2013, 106)\n\nLet’s assume \\(f(x) = 3x\\). To compute the discrete rate of change between any two points, we look at the amount of change on the \\(y\\)-axis relative to a particular amount of change on the \\(x\\)-axis:\n\\[\nm = \\dfrac{ f(x_2) - f(x_1) }{ x_2 - x_1 }\n\\]\nThis is the equation of the slope of a line.\n\nThe rate of change between two discrete points is just the slope of the line connecting those two points, known as a secant. (Moore and Siegel 2013, 107)\n\n\nThe derivative of a function is the instantaneous rate of change at a point. As noted in the previous section, we can take the difference between two points and calculate the discrete rate of change (i.e., the rate of change between any two points). But the derivative is the continuous rate of change (i.e., the instantaneous rate of change at any given point). (Moore and Siegel 2013, 110–11)\n\nWe can compute the derivative the following way:\n\\[\n\\underset{h \\rightarrow 0}{\\text{lim}} \\dfrac{ f(x+h) - f(x) }{h} = f^\\prime (x) = \\dfrac{d}{dx} f(x) = \\dfrac{dy}{dx}\n\\]\n\nThe only thing missing from this formal presentation is the existence of derivatives. Derivatives can only be calculated at points at which limits exist, because derivatives are in a sense limits themselves, but it is also necessary for a function to be continuous at a point to have a derivative at that point.\n\n\n\nThe Rules of Differentiation, Moore and Siegel (2013)\nSolução de alguns dos exercícios do capítulo.\nThe derivative is a linear operator, what implies that \\(\\frac{ d(f+g) }{dx} = \\frac{df}{dx} + \\frac{dg}{dx}\\) and \\(\\frac{d(cf)}{dx} = c \\frac{df}{dx}\\).\n\nRecall from Chapter 3 that a composite function looks like g(f(x)). To see why we’re bringing this up now, note that most complex functions can be written as composite functions. […]. Thus, we can break complicated functions down into the composition of simpler functions. If we can devise a rule for differentiating composite functions, a rule known as the chain rule, then we can simplify the differentiation of complex functions immensely. (Moore and Siegel 2013, 119)\n\n[chain rule]: \\((g(f(x)))^\\prime = g^\\prime (f(x)) f^\\prime (x)\\)\n[product rule]: \\(\\dfrac{ (f(x) g(x)) }{ dx } = \\dfrac{ d f(x) }{dx} g(x) + f(x) \\dfrac{ dg(x) }{dx}\\)\n[quotient rule]: \\(\\dfrac{d}{dx} \\dfrac{f(x)}{g(x)} = \\dfrac{ \\frac{df(x)}{dx} g(x) - f(x) \\frac{dg(x)}{dx} }{ g(x)^2 }\\)\n[power rule]: \\(\\dfrac{d x^n}{dx} = n x^{n-1}\\)\n[exponentials]: \\(\\dfrac{d e^x}{dx} = e^x\\), and \\(\\dfrac{d a^x}{dx} = a^x (\\ln{(a)})\\)\n[logarithms]: \\(\\dfrac{d \\ln{(x)}}{dx} = \\dfrac{1}{x}\\), and \\(\\dfrac{d \\log_a{(x)}}{dx} = \\dfrac{1}{x \\ln{(a)}}\\)\n\n\nAnotações de aula\n“Notebook notes”, com um aprofundamento em cálculo, neste link. As demais partes da aula trataram brevemente da ideia de processo gerador de dados.\n\nset.seed(42)\n\nVamos simular o processo gerador de dados (data generating process) de renda\n\nn = 1000\nerros = rnorm(n, mean = 0, sd = 200)\neduc = rpois(n, lambda = 7)\n\n# a distribuição dos erros é normal com média 0 e sd 200\nhist(erros)\n\n\n\n\n\n\n\n# processo gerador de dados\nrenda = 1000 + 150*educ + erros\n\nDe fato, se estimarmos uma reta de regressão utilizando esses dados, devemos nos aproximar bem dos coeficientes definidos no processo gerador:\n\nlm(renda ~ educ)\n\n\nCall:\nlm(formula = renda ~ educ)\n\nCoefficients:\n(Intercept)         educ  \n     1004.0        148.7  \n\n\nAgora, vamos criar uma nova variável que, predeterminadamente, já está correlacionada com os nossos erros. Isto é, trata-se de uma especificação que viola o ML4 (conforme vimos na última aula). Isso desrespeita a independência entre os erros e os regressores.\n\nlibrary(faux)\n\nWarning: pacote 'faux' foi compilado no R versão 4.4.3\n\n\n\n************\nWelcome to faux. For support and examples visit:\nhttps://debruine.github.io/faux/\n- Get and set global package options with: faux_options()\n************\n\nn = 1000\nerros = rnorm(n, mean = 0, sd = 200)\neduc = rnorm_pre(erros, mu = 7, r = -.8)\nrenda = 1000 + 150*educ + erros\n\nlm(renda ~ educ)\n\n\nCall:\nlm(formula = renda ~ educ)\n\nCoefficients:\n(Intercept)         educ  \n   2116.673       -8.889  \n\n\nQuando a regressão estimada não respeita os pressupostos, os resultados obtidos são absolutamente diversos em relação ao processo que efetivamente gerou o dado. O estimador utilizado para fazer a estimativa aplica uma assumption que o seu dado não respeitava.\nNa regressão, temos uma espécie de “máquina” para representar o mecanismo das coisas. O que significa essa estimativa, então? Essa resposta está no cálculo; por isso, precisamos falar um pouco dele antes.\n\n\n\n\nMoore, Will H., and David A. Siegel. 2013. A Mathematics Course for Political and Social Research. Princeton, NJ: Princeton University Press.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Um pouco mais de cálculo</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html",
    "href": "aulas/aula-3.html",
    "title": "5  O método dos mínimos quadrados",
    "section": "",
    "text": "Extrema in One Dimension, Moore and Siegel (2013)\nSolução de alguns dos exercícios do capítulo.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>O método dos mínimos quadrados</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#higher-order-derivatives",
    "href": "aulas/aula-3.html#higher-order-derivatives",
    "title": "5  O método dos mínimos quadrados",
    "section": "5.1 Higher-order derivatives",
    "text": "5.1 Higher-order derivatives\n\nMore generally, though, derivatives of any order tell us something about the shape of the function. The first derivative tells us in which diretion it is trending: is it increasing or decreasing? The second derivative tells us the most basic curvature of the function: is the rate at which it is increasing or decreasing getting faster or slower? Higher-order derivatives provide more nuanced information about the shape of functions; however, we will typically find first and second derivatives sufficient for our purposes. (Moore and Siegel 2013, 158)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>O método dos mínimos quadrados</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#concavity-and-convexity",
    "href": "aulas/aula-3.html#concavity-and-convexity",
    "title": "5  O método dos mínimos quadrados",
    "section": "5.2 Concavity and Convexity",
    "text": "5.2 Concavity and Convexity\n\nTo understand what the second derivative tells us, let’s start by considering an increasing function. One with a rate of increase that slows as the value of the function gets bigger is an example of a concave function. One with a rate of increase that speeds up as the value of the function gets bigger is an example of a convex function. (Moore and Siegel 2013, 159)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>O método dos mínimos quadrados</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#taylor-series",
    "href": "aulas/aula-3.html#taylor-series",
    "title": "5  O método dos mínimos quadrados",
    "section": "5.3 Taylor Series",
    "text": "5.3 Taylor Series\n\nThe first derivative tells us whether the function is increasing or decreasing, the second what the curvature is, and so on. This suggests that one could build up a function by incorporating all the information encoded in these derivatives. It turns out that one can do this for a large class of functions known as analytic functions. The series that expresses a function in terms of its derivatives is known as a Taylor series, named after the English mathematician Brook Taylor. If an analytic function \\(f(x)\\) is infinitely differentiable close to some number \\(a\\), then the Taylor series is given by the infinite sum\n\\[\n\\begin{align*}\nf(x) &= f(a) + \\dfrac{f^\\prime (a)}{1!}(x-a) + \\dfrac{f^{\\prime \\prime} (a)}{2!}(x-a)^2 + \\dfrac{f^{\\prime \\prime \\prime} (a)}{3!}(x-a)^3 + \\dots \\\\\n&= \\sum^\\infty_{n = 0} \\dfrac{f^{(n)} (a)}{n!} (x - a)^n\n\\end{align*}\n\\]\nThe Taylor series is useful for many reasons, but the primary one for our purposes is that it allows one to replace a complex function with a bunch of powers of \\(x\\). We already used this to calculate the derivative of \\(e^x\\) […]. (Moore and Siegel 2013, 160–61)\n\n\nMore usefully, the Taylor series provides a good approximation of a function near any point. Note that the expansion is in powers of \\((x-a)\\). If we only care about values of \\(x\\) near \\(a\\), an so consider only these, then \\((x-a)\\) is small, \\((x-a)^2\\) is smaller, and so on. At some point, adding an additional term doesn’t change the sum enough to be worth it, particularly with the \\(n!\\) in the denominator. Se we can cut off the approximation there. (Moore and Siegel 2013, 162)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>O método dos mínimos quadrados</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#critical-points",
    "href": "aulas/aula-3.html#critical-points",
    "title": "5  O método dos mínimos quadrados",
    "section": "5.4 Critical points",
    "text": "5.4 Critical points\n\nA critical point is any point \\(x^*\\) such that either \\(f^\\prime (x^*) = 0\\) or \\(f^\\prime (x^*)\\) doesn’t exist. Loosely, critical points are points in the function’s domain at which things happen. Either the function blows up, or it jumps, or it is stationary. (Moore and Siegel 2013, 162)\n\n\nHowever, just because local extrema occur at critical points does not mean all critical points are extrema. Some are instead inflection points, which are points at which the graph of the function changes from concave to convex or vice versa. For example, up to a certain point a function may be increasing at a slower and slower rate, but after that point it might increase at a faster and faster rate. Such a function if \\(f(x) = x^3\\), which has an inflection point at \\(x = 0\\), even though \\(f^\\prime (0) = 0\\). (Moore and Siegel 2013, 162)\n\n\nWhen the slope of the tangent line to the function is not zero at the inflection point, the existence of inflection points gives us no trouble in finding extrema. Given that our interest is in finding extrema and not inflection points, we needn’t concern ourselves with these (called nonstationary points of inflection). However, as in our example, in some instances the slope of a tangent to the inflection point will equal zero. That is, both \\(f^\\prime (x^*) = 0\\) and \\(f^{\\prime \\prime} (x^*) = 0\\), and \\(x^*\\) is further an inflection point. Such points are also known as saddle points (owing to their appearance in two dimensions) and are not extrema. (Moore and Siegel 2013, 163)\n\n\nWe use the second derivative test to determine whether the stationary points we obtained in the first derivative test are extrema or inflection points. First we have to determine the second derivative, \\(f^{\\prime \\prime} (x)\\), of the original function \\(f(x)\\). Then we substitute the stationary points \\(x^*\\) we determined from the FOC into \\(f^{\\prime \\prime} (x)\\). If the answer is negative, i.e., if \\(f^{\\prime \\prime} (x^*) &lt; 0\\), the stationary point is a maximum since the function is concave near \\(x^*\\). If, in contrast, \\(f^{\\prime \\prime} (x^*) &gt; 0\\), the stationary point is aminimum, since the function is convex near \\(x^*\\). Finally, if \\(f^{\\prime \\prime} (x^*) = 0\\), the stationary point may be an inflection point.\n\nIn the case of \\(f^{\\prime \\prime} (x^*) = 0\\), we must take the third derivative to check if \\(x^*\\) is an inflection point or not. An inflection point occurs whenever the sign of the second derivative changes, since it implies a shift from convex to concave, or vice versa. If \\(f(x) = x^3\\), \\(f^{\\prime \\prime \\prime}(x) = 6 &gt; 0\\), so zero is an inflection point, as we have found. However, if \\(f(x) = x^4\\), \\(f^{\\prime \\prime \\prime}(x) = 24x\\), which is \\(0\\) at \\(x = 0\\), so this is not necessarily an inflection point.\nExample: \\(f(x) = x^3 - 3x^2 + 7\\). The first derivative is \\(f^\\prime (x) = 3x^2 - 6x\\), which has \\(x^* = 0\\) and \\(x^* = 2\\) as extreme points. Something is happening there. So let’s take the derivative again: \\(f^{\\prime \\prime} (x) = 6x - 6\\). If we plug \\(x^* = 0\\) into it, we get \\(-6 &lt; 0\\), which means that \\(x^* = 0\\) is a local maximum of the function. \\(x^* = 2\\), instead, gives us \\(6 &gt; 0\\), what makes that point a local minimum.\nWell, and how do we find globals? Suppose we are in the interval \\([-4, 4]\\). The local minimum of this very function is \\(f(2) = 3\\), and the local maximum is \\(f(0) = 7\\). Given the domain, we know the boundaries are \\(f(-4) = -105\\) and \\(f(4) = 23\\). Since \\(-105 &lt; 3\\) and \\(23 &gt; 7\\), the global minimum and maximum are at \\(x = -4\\) and \\(x = 4\\). (Moore and Siegel 2013, 168)\nProcedure to find global maximum and minimum:\n\nFind \\(f^\\prime (x)\\)\nSet \\(f^\\prime (x) = 0\\) and solve for \\(x\\) to obtain stationary points\nFind \\(f^{\\prime \\prime} (x)\\)\nFor each stationary point \\(x^*\\), substitute it into \\(f^{\\prime \\prime} (x)\\)\n\n\nif \\(f^{\\prime \\prime} (x^*) &lt; 0\\), \\(f(x)\\) has a local maximum at \\(x^*\\)\nif \\(f^{\\prime \\prime} (x^*) &gt; 0\\), \\(f(x)\\) has a local minimum at \\(x^*\\)\nif \\(f^{\\prime \\prime} (x^*) = 0\\), \\(x^*\\) may be an inflection point\n\nThis goes on until you find a higher-order derivative for which plugging \\(x^*\\) returns a non-zero value\n\n\n\nSubstitute each local extremum into \\(f(x)\\) to find the function’s value at that point\nSubstitute the lower and upper bounds of the domain over which you are attempting to find the extrema into \\(f(x)\\) to find the function’s value at those points\nFind the smallest value of the function from those computed in the previous two steps. This is the global minimum, and the function attains this at the corresponding \\(x^*\\) or boundary point. Find the largest value of the function from those computed in the previous two steps. This is the global maximum, and the function attains this at the corresponding \\(x^*\\) or boundary point.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>O método dos mínimos quadrados</span>"
    ]
  },
  {
    "objectID": "aulas/bibliography.html",
    "href": "aulas/bibliography.html",
    "title": "6  Bibliografia",
    "section": "",
    "text": "Moore, Will H., and David A. Siegel. 2013. A Mathematics Course for\nPolitical and Social Research. Princeton, NJ: Princeton University\nPress.\n\n\nStrang, Gilbert. 2016. Introduction to Linear Algebra. 5th ed.\nWellesley, MA: Wellesley-Cambridge Press.\n\n\nWooldridge, Jeffrey M. 2020. Introductory Econometrics: A Modern\nApproach. 7th ed. Boston, MA: Cengage Learning.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bibliografia</span>"
    ]
  }
]